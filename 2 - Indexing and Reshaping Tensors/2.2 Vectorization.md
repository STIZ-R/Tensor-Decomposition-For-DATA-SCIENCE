# 2.2 Vectorization

The **vectorization** tranforms a tensor into a vector. It is important to understand since it's related on how the tensor is stored in computer memory.

## 2.2.1 Vectorizing 3-way Tensors

Take a 3-way tensor $X$ of size $m \times n \times p$, the columnar vector of size $q = mnp$ is $vec(X)$.

```math
vec(X) = \begin{bmatrix} x_{111} \\ x_{211} \\ . \\ . \\ . \\ x_{mnp} \end{bmatrix} = \qquad \begin{bmatrix} x_1 \\ x_2 \\ . \\. \\ . \\ x_q \end{bmatrix}
```
Figure 1
*(from tuple indices to linear indices with q = mnp)*

To see it easly, let take the following example:
```math
X(;,;,1) = \begin{bmatrix} 3 & 9 & 1 \\ 8 & 2 & 1 \\ 4 & 3 & 9 \end{bmatrix}, X(;,;,2) = \begin{bmatrix} 6 & 9 & 5 \\ 5 & 6 & 4 \\ 1 & 4 & 1 \end{bmatrix}
```
It's simply to follow each slices in order. So the result would be 
```math
vec(X)^T = \begin{bmatrix}3 & 8 & 4 & 9 & 2 & 3 & 1 & 1 & 9 & 6 & 5 & 1 & 9 & 6 & 4 & 5 & 4 & 1\end{bmatrix}.
```
*(horizontaly for clarity)*

<br> <br>

## 2.2.2 Vectorizing *d*-way Tensors

> **Definition (Vectorization of *d*-way Tensor)** <br>
> Let $X$ be a tensor of size $n_1 \times n_2 \times \dots \times n_d$. <br>
> Its **vectorization**, $vec(X)$, is a column vector of length $N = \prod_{k=1}^dn_k$ such that entry $\alpha \in [N]$ is defined as $\alpha = \mathbb{L}(i_1, i_2, \dots, i_d)$ and, conversely, $(i_1, i_2, \dots, i_d) = \mathbb{T}(\alpha)$ so that <br>
```math
\begin{aligned}
vec(X) = \begin{bmatrix} x_{11\dots1} \\ x_{21\dots1} \\ . \\ . \\ . \\ x_{n_1n_2\dots n_d} \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \\ . \\ . \\ . \\ x_N \end{bmatrix}
\\[8pt]
\text{With } N = \prod_{k=1}^dn_k.
\end{aligned}
```

I will not show the example of a tensor of size $3 \times 4 \times 3 \times 2$. (too long)

<br> <br>

## 2.2.3 Representing Tensors in Computer Memory

Storing multidimensional arrays into arrays of arrays or lists of lists is kind of inneficient.
Indeed, the best way is to store a tensor as a contiguous one-dimensional array ($vec(X)$). Since computer memory is linear, if we know the location of $X(1,1, \dots, 1)$, we can find the location of element $(i_1,i_2, \dots, i_d)$ by just looking at the $(\mathbb{L}(i_1,i_2, \dots, i_d) - 1)$ spot in the memory.

<br> <br> 
## Exercises 

> ** Exercise 2.21** <br>
> Show that the figure 1 is identical to a matrix vectorization for an $m \times n \times p$ tensor with $p=1$

### Solution - Exercise 2.21

We know that the natural linear index for $m \times n \times p$ is $l = 1 + (i - 1) + m(j - 1) + mn(k - 1)$ and if $p=1$, then $k = 1$ so, it's equal to $l = 1 + (i - 1) + m(j - 1)$. Which is the same thing as a matrix of size $m \times n$.

<br> <br>

> **Exercise 2.22** <br>
> Let the $2 \times 2 \times 2$ tensor $X$ given by the figure 2. <br>
> What is $vec(X)$?

> Figure 2
```math
  X(;,;,1) = \begin{bmatrix} 8 & 7 \\ -3 & 9 \end{bmatrix}, \qquad X(;,;,2) = \begin{bmatrix} -1 & 4 \\ 0 & 5 \end{bmatrix}
```

### Solution - Exercise 2.22

```math
vec(X)^T = \begin{bmatrix} 8 & -3 & 7 & 9 & -1 & 0 & 4 & 5 \end{bmatrix}
```
*(horizontaly for clarity)*
