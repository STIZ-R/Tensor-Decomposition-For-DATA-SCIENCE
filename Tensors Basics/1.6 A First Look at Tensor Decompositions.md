## 1.6 A First Look at Tensor Decompositions

<br>

Tensor decompositions can be seen as a set of tensors that can be multiplied together to reconstruct the initial input. <br>
Unlike matrix decompositions, tensor decompositions result in an approximation of the input. <br>
The majority of tensor decompositions are viewed as a representation of low-rank matrix approximations for higher-order data.

<br>

### 1.6.1 A First Look at Tucker Decomposition

The **Tucker Decomposition** is a method which compresses a tensor by decomposing it into a smaller **core tensor** multiplied by a matrix for each mode of the original tensor.

Given a tensor $\mathcal{X} \in \mathbb{R}^{n_1 \times \dots \times n_N}$,  
its Tucker decomposition is
```math
\mathcal{X} = \mathcal{G} \times_1 A^{(1)} \times_2 A^{(2)} \dots \times_N A^{(N)},
```
where $\mathcal{G}$ is the core tensor and $A^{(k)}$ are factor matrices along each mode.

```text
                                            +------+
                                          /  W    /
                                        +-------+
  +--------------+                   +-------+   +---------------+
 /              /|      +-------+  +-------+ |   |               |
+-------------+  |      |       |  |       | +   |       V       |
|             |  |  ≈   |       |  |   G   |/    +---------------+
|      X      |  |      |   U   |  +-------+          
|             |  +      |       |             
|             | /       |       |         
+-------------+         +-------+

           a) Tucker Decomposition   

```
It is possible to choose the size of the **core** to ensure we have a minimalist error.

<br>

### 1.6.2 A First Look at CP Decomposition

The **CP decomposition** expresses a tensor as a sum of rank-1 tensors, i.e., vector outer products.  
The summands are called **components**, and the vectors in each component are used for *interpretation*.

Given a 3-way tensor $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$, its CP decomposition writes
```math
\mathcal{X} \approx \sum_{r=1}^R a_r \circ b_r \circ c_r,
```

```text

                                +  c1                    + c2                           + cr
                              /                        /                              /
  +--------------+           /                       /                              /
 /              /|         +----------+ b1          +----------+ b2                +----------+ br 
+-------------+  |         |                        |                              |
|             |  |  ≈  λ_1 |                 +  λ_2 |                + . . . + λ_r |                 
|      X      |  |         |                        |                              |
|             |  +         |                        |                              |
|             | /          |                        |                              |
+-------------+            + a1                     + a2                           + ar

           a) Tucker Decomposition   

```
