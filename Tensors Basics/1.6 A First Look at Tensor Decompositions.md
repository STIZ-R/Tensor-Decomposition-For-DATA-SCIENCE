## 1.6 A First Look at Tensor Decompositions

<br>

Tensor decompositions can be seen as a set of tensors that can be multiplied together to reconstruct the initial input. <br>
Unlike matrix decompositions, tensor decompositions result in an approximation of the input. <br>
The majority of tensor decompositions are viewed as a representation of low-rank matrix approximations for higher-order data.

<br>

### 1.6.1 A First Look at Tucker Decomposition

The **Tucker Composition** is a method which compresses a tensor by decomposing it into a smaller **core tensor** multiplied by a matrix for each mode of the original tensor.

Given a tensor $\mathcal{X} \in \mathbb{R}^{n_1 \times \dots \times n_N}$,  
its Tucker decomposition is
\[
\mathcal{X} = \mathcal{G} \times_1 A^{(1)} \times_2 A^{(2)} \dots \times_N A^{(N)},
\]
where $\mathcal{G}$ is the core tensor and $A^{(k)}$ are factor matrices along each mode.


<br>

### 1.6.2 A First Look at CP Decomposition
